# ‚úÖ API Reference - Validators

Documentation compl√®te des validateurs de m√©triques d'**Arkalia Metrics Collector**.

## üéØ Vue d'ensemble

Les validateurs permettent de v√©rifier la qualit√©, la coh√©rence et la validit√© des m√©triques collect√©es.

## üì¶ MetricsValidator

Classe principale pour la validation des m√©triques collect√©es.

### Constructeur

```python
MetricsValidator()
```

**Aucun param√®tre requis** - Configuration par d√©faut

### Attributs

- `validation_errors` : Liste des erreurs de validation
- `validation_warnings` : Liste des avertissements de validation

### M√©thodes principales

#### validate_metrics(metrics_data: dict[str, Any]) -> tuple[bool, list[str], list[str]]

Valide les m√©triques collect√©es.

**Param√®tres :**
- `metrics_data` : Donn√©es des m√©triques √† valider

**Retour :** `tuple[bool, list[str], list[str]]` - (is_valid, errors, warnings)

**Exemple :**
```python
validator = MetricsValidator()
is_valid, errors, warnings = validator.validate_metrics(metrics_data)

if is_valid:
    print("‚úÖ M√©triques valides!")
else:
    print(f"‚ùå {len(errors)} erreurs d√©tect√©es")
```

#### get_validation_report() -> dict[str, Any]

G√©n√®re un rapport de validation d√©taill√©.

**Retour :** `dict` avec :
- `errors` : Liste des erreurs
- `warnings` : Liste des avertissements
- `validation_summary` : R√©sum√© avec score et statut

**Exemple :**
```python
report = validator.get_validation_report()
print(f"Score: {report['validation_summary']['score']}/100")
print(f"Statut: {report['validation_summary']['status']}")
```

### R√®gles de validation

#### Structure des m√©triques

**Cl√©s requises :**
- `timestamp` : Horodatage de la collecte
- `project_root` : Chemin racine du projet
- `collection_info` : Informations sur la collecte
- `python_files` : M√©triques des fichiers Python
- `test_metrics` : M√©triques des tests
- `documentation_metrics` : M√©triques de la documentation
- `summary` : R√©sum√© des m√©triques

**Informations de collection requises :**
- `collector_version` : Version du collecteur
- `python_version` : Version Python utilis√©e
- `collection_date` : Date de collecte

#### Coh√©rence des donn√©es

**Validation Python :**
- `core_files + test_files == count` (coh√©rence du comptage)
- `total_lines > 0` si `count > 0` (lignes de code pr√©sentes)

**Validation des tests :**
- `test_files_count == python_test_files` (coh√©rence du comptage)
- `collected_tests_count >= 0` (nombre de tests non n√©gatif)

**Validation de la documentation :**
- `documentation_files >= 0` (nombre de fichiers non n√©gatif)

**Validation du r√©sum√© :**
- `total_python_files` coh√©rent avec `python_files.count`
- `lines_of_code` coh√©rent avec `python_files.total_lines`
- `collected_tests` coh√©rent avec `test_metrics.collected_tests_count`

### Messages d'erreur et d'avertissement

#### Erreurs (validation_errors)

**Structure :**
- `"Cl√© manquante: {key}"` : Cl√© requise manquante
- `"Nombre de fichiers Python manquant"` : M√©trique Python manquante
- `"Incoh√©rence dans le comptage des fichiers: {core} + {test} != {count}"` : Comptage incoh√©rent
- `"Incoh√©rence dans le comptage des tests: {count} != {python_count}"` : Tests incoh√©rents
- `"Nombre de tests n√©gatif: {count}"` : Valeur n√©gative
- `"Nombre de fichiers de documentation n√©gatif: {count}"` : Valeur n√©gative
- `"Incoh√©rence dans le r√©sum√©: {expected} != {actual}"` : R√©sum√© incoh√©rent

#### Avertissements (validation_warnings)

**Structure :**
- `"Info de collection manquante: {key}"` : Information optionnelle manquante
- `"Fichiers Python d√©tect√©s mais 0 lignes de code"` : Probl√®me potentiel
- `"Aucun fichier Python d√©tect√©"` : Projet vide
- `"Aucun fichier de documentation d√©tect√©"` : Documentation manquante

### Score de validation

Le score est calcul√© automatiquement :
- **Base** : 100 points
- **Erreur** : -10 points par erreur
- **Avertissement** : -2 points par avertissement
- **Minimum** : 0 points

**Statuts :**
- `"‚úÖ VALID"` : Score = 100, aucune erreur
- `"‚ùå INVALID"` : Score < 100, erreurs d√©tect√©es

### Exemples d'utilisation

#### Validation basique

```python
from arkalia_metrics_collector import MetricsValidator

validator = MetricsValidator()
is_valid, errors, warnings = validator.validate_metrics(metrics_data)

print(f"Valid: {is_valid}")
print(f"Erreurs: {len(errors)}")
print(f"Avertissements: {len(warnings)}")
```

#### Validation avec rapport

```python
validator = MetricsValidator()
validator.validate_metrics(metrics_data)
report = validator.get_validation_report()

print("=== RAPPORT DE VALIDATION ===")
print(f"Score: {report['validation_summary']['score']}/100")
print(f"Statut: {report['validation_summary']['status']}")

if report['errors']:
    print("\n‚ùå ERREURS:")
    for error in report['errors']:
        print(f"  - {error}")

if report['warnings']:
    print("\n‚ö†Ô∏è AVERTISSEMENTS:")
    for warning in report['warnings']:
        print(f"  - {warning}")
```

#### Validation dans un pipeline

```python
def validate_project_metrics(project_path: str) -> bool:
    """Valide les m√©triques d'un projet."""
    try:
        # Collecter les m√©triques
        collector = MetricsCollector(project_path)
        metrics = collector.collect_all_metrics()
        
        # Valider
        validator = MetricsValidator()
        is_valid, errors, warnings = validator.validate_metrics(metrics)
        
        # Afficher le rapport
        if not is_valid:
            print(f"‚ùå Projet {project_path} invalide:")
            for error in errors:
                print(f"  - {error}")
        
        return is_valid
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la validation: {e}")
        return False

# Utilisation
projects = ["./projet1", "./projet2", "./projet3"]
for project in projects:
    if validate_project_metrics(project):
        print(f"‚úÖ {project} valid√©")
    else:
        print(f"‚ùå {project} invalide")
```

### Int√©gration avec d'autres composants

#### Collecteur + Validateur

```python
# Collecter et valider en une fois
collector = MetricsCollector("./mon-projet")
validator = MetricsValidator()

# Collecter
metrics = collector.collect_all_metrics()

# Valider imm√©diatement
is_valid, errors, warnings = validator.validate_metrics(metrics)

if is_valid:
    print("‚úÖ M√©triques collect√©es et valid√©es!")
else:
    print("‚ùå M√©triques collect√©es mais invalides")
```

#### Exporteur + Validateur

```python
# Valider avant l'export
validator = MetricsValidator()
is_valid, errors, warnings = validator.validate_metrics(metrics_data)

if is_valid:
    # Exporter seulement si valide
    exporter = MetricsExporter(metrics_data)
    exporter.export_all_formats("output/")
    print("‚úÖ Export r√©ussi!")
else:
    print("‚ùå Export annul√© - m√©triques invalides")
```

### Personnalisation future

**R√®gles personnalis√©es :**
- Seuils de qualit√© configurables
- R√®gles m√©tier sp√©cifiques
- Validation conditionnelle

**Extensions :**
- Validation de sch√©mas JSON
- R√®gles de qualit√© de code
- M√©triques de performance
