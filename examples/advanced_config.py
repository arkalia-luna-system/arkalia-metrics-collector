#!/usr/bin/env python3
"""
Exemple d'utilisation avancÃ©e avec configuration personnalisÃ©e
=============================================================

Ce script montre comment utiliser le collecteur avec des configurations avancÃ©es.
"""

import yaml
from pathlib import Path
from arkalia_metrics_collector import MetricsCollector, MetricsExporter


def create_custom_config():
    """CrÃ©e une configuration personnalisÃ©e."""
    config = {
        "project": {
            "name": "Mon Projet Awesome",
            "type": "web_application",
            "description": "Application web Django avec API REST"
        },
        "exclusions": [
            "**/migrations/**",
            "**/static/vendor/**", 
            "**/media/**",
            "**/locale/**",
            "**/venv/**",
            "**/.venv/**",
            "**/node_modules/**",
            "**/__pycache__/**",
            "**/.pytest_cache/**",
            "**/htmlcov/**",
            "**/coverage/**"
        ],
        "metrics": {
            "include_tests": True,
            "include_documentation": True,
            "deep_analysis": False
        }
    }
    
    config_path = Path("custom_metrics_config.yaml")
    with open(config_path, "w", encoding="utf-8") as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"ğŸ“‹ Configuration crÃ©Ã©e : {config_path}")
    return config_path


def analyze_multiple_projects():
    """Analyse plusieurs projets et compare les mÃ©triques."""
    projects = [
        {"name": "frontend", "path": "./frontend"},
        {"name": "backend", "path": "./backend"}, 
        {"name": "shared", "path": "./shared"},
        {"name": "docs", "path": "./docs"}
    ]
    
    all_metrics = {}
    
    for project in projects:
        print(f"\nğŸ” Analyse de {project['name']}...")
        
        # VÃ©rifier si le projet existe
        project_path = Path(project["path"])
        if not project_path.exists():
            print(f"âš ï¸  Projet {project['name']} non trouvÃ© Ã  {project_path}")
            continue
        
        # Collecter les mÃ©triques
        collector = MetricsCollector(project["path"])
        metrics = collector.collect_all_metrics()
        all_metrics[project["name"]] = metrics
        
        # Afficher rÃ©sumÃ©
        summary = metrics.get("summary", {})
        print(f"   ğŸ Python: {summary.get('total_python_files', 0)} fichiers")
        print(f"   ğŸ“ Code: {summary.get('lines_of_code', 0):,} lignes")
        print(f"   ğŸ§ª Tests: {summary.get('collected_tests', 0)} tests")
    
    return all_metrics


def generate_comparison_report(all_metrics):
    """GÃ©nÃ¨re un rapport de comparaison entre projets."""
    print("\nğŸ“Š GÃ©nÃ©ration du rapport de comparaison...")
    
    # Calculer les totaux
    total_files = sum(
        m.get("summary", {}).get("total_python_files", 0) 
        for m in all_metrics.values()
    )
    total_lines = sum(
        m.get("summary", {}).get("lines_of_code", 0)
        for m in all_metrics.values()
    )
    total_tests = sum(
        m.get("summary", {}).get("collected_tests", 0)
        for m in all_metrics.values()
    )
    
    # CrÃ©er le rapport
    report = f"""# ğŸ“Š Rapport de Comparaison Multi-Projets

## ğŸ¯ Vue d'ensemble

- **Total fichiers Python**: {total_files:,}
- **Total lignes de code**: {total_lines:,}
- **Total tests**: {total_tests:,}

## ğŸ“ˆ DÃ©tails par projet

"""
    
    for project_name, metrics in all_metrics.items():
        summary = metrics.get("summary", {})
        files = summary.get("total_python_files", 0)
        lines = summary.get("lines_of_code", 0)
        tests = summary.get("collected_tests", 0)
        docs = summary.get("documentation_files", 0)
        
        # Calculer les pourcentages
        files_pct = (files / total_files * 100) if total_files > 0 else 0
        lines_pct = (lines / total_lines * 100) if total_lines > 0 else 0
        tests_pct = (tests / total_tests * 100) if total_tests > 0 else 0
        
        report += f"""### ğŸ¯ {project_name.title()}

| MÃ©trique | Valeur | % du total |
|----------|--------|------------|
| Fichiers Python | {files:,} | {files_pct:.1f}% |
| Lignes de code | {lines:,} | {lines_pct:.1f}% |
| Tests | {tests:,} | {tests_pct:.1f}% |
| Documentation | {docs:,} | - |

"""
    
    # Sauvegarder le rapport
    report_path = Path("comparison_report.md")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"ğŸ“„ Rapport sauvegardÃ© : {report_path}")
    return report_path


def main():
    """Exemple d'utilisation avancÃ©e."""
    print("ğŸš€ Arkalia Metrics Collector - Utilisation AvancÃ©e")
    print("=" * 50)
    
    # 1. CrÃ©er une configuration personnalisÃ©e
    config_path = create_custom_config()
    
    # 2. Analyser le projet courant avec config personnalisÃ©e
    print("\nğŸ” Analyse avec configuration personnalisÃ©e...")
    collector = MetricsCollector(".")
    metrics = collector.collect_all_metrics()
    
    # 3. Export avec noms personnalisÃ©s
    exporter = MetricsExporter(metrics)
    timestamp = metrics.get("timestamp", "").replace(":", "-").split(".")[0]
    
    output_dir = Path(f"advanced_metrics_{timestamp}")
    output_dir.mkdir(exist_ok=True)
    
    # Export personnalisÃ©
    exports = {
        "Rapport principal": exporter.export_json(str(output_dir / "full_metrics.json")),
        "RÃ©sumÃ© exÃ©cutif": exporter.export_markdown_summary(str(output_dir / "executive_summary.md")),
        "Dashboard interactif": exporter.export_html_dashboard(str(output_dir / "interactive_dashboard.html")),
        "DonnÃ©es pour Excel": exporter.export_csv(str(output_dir / "data_for_excel.csv"))
    }
    
    print(f"\nğŸ’¾ Exports dans {output_dir}:")
    for name, success in exports.items():
        status = "âœ…" if success else "âŒ"
        print(f"   {status} {name}")
    
    # 4. Analyser plusieurs projets (optionnel)
    try:
        all_metrics = analyze_multiple_projects()
        if all_metrics:
            generate_comparison_report(all_metrics)
    except Exception as e:
        print(f"âš ï¸  Analyse multi-projets ignorÃ©e : {e}")
    
    # 5. Nettoyage
    if config_path.exists():
        config_path.unlink()
        print(f"\nğŸ§¹ Configuration temporaire supprimÃ©e")
    
    print(f"\nğŸ‰ Analyse avancÃ©e terminÃ©e !")
    print(f"ğŸ“Š RÃ©sultats dans : {output_dir.absolute()}")


if __name__ == "__main__":
    main()
